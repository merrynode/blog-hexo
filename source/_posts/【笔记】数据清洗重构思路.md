---
title: 【笔记】数据清洗重构思路
date: 2017-03-12 19:41:27
tags: [node, 爬虫, 清洗]
type: javaScript
comments: true
categories: 心得笔记
---
> 由于重构了抓取服务，现在能对用户的大交易量订单抓取，需要清洗的文件越来越大了，所以最近清洗服务出现问题，【单用户500多万条订单以上，大概1.58个G，最大的一个用户10多个G】洗着洗着进程就炸了。由于清洗服务是从一年多前老项目复制过来的，清洗速度实在是不敢恭维，而且还有些许逻辑问题，所以小组内打算对清洗服务进行重构。

>老的清洗逻辑是将文件逐行读取到存到对象里，每次读取都判断一下对象是否已存在该订单，如果有，则接着判断订单时间，以最新的订单时间的状态为准，更新该订单，这就导致数据量过大内存直接爆炸，即使设置了 `--max-old-space-size` 和 `--max-new-space-size` 也扛不住。而且每次获取对象的属性值效率是很慢的。注： JavaScript 获取数据的性能有如下顺序（从快到慢）：变量获取 > 数组下标获取（对象的整数索引获取） > 对象属性获取（对象非整数索引获取），但实际测试下来差距并不是很大，几乎可以忽略不计。

>单纯的将数据存在内存的方式并不可取，特别是碰到大数据的客户的时候，所以将清洗服务分成了两部分，当数据量小的时候走内存，当数据量大的时候用新的清洗方式，如下图：
![清洗流程图](http://i1.piimg.com/567571/d7f513cbb2d31915.png)

>如果将订单都存到同一DB会导致redis检索十分缓慢，所以将订单按订单号分成99组，根据订单号后两位将订单存入对应的DB中，等最终清洗完所有数据后再将DB中的数据依次输出到out文件, 实际测试后分库之后的效率比不分库提升了5倍多。

